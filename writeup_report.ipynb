{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Model Architecture and Training Strategy:\n",
    "\n",
    "   My first attempt at the project used the CNN built during the lesson. It turned out to be insufficient. Subsequent attempts centered around collecting more driving data. However, greater amounts of data did not improve the performance. After a couple of days I slapped my forehead and cried, \"D'oh!\" I had been writing the new data to a different location. Thus I was getting no improvement as I was running the same old data set each time. In the mean time I had decided that the problem lay with the CNN. So, I changed mine to be the one in the Nvidia paper. I have no idea if the old CNN would have worked. \n",
    "   To minimize the compute cycles needed I cropped the images by 20 pixels on each side, 20 on the bottom (removing the hood of the car), and 75 on the top (removing the sky and trees).\n",
    "   I used three convolution layers of depths 16, 32, and 64. Each with 3X3 filters. After each convolution I used a max pooled layer. I flattened the output the used reducing Dense layers of 400, 100, 20, and 1. \n",
    "   To reduce nonlinearity I used activated RELU on each convolution and each Dense except the last one.\n",
    "   ADDENDUM: \n",
    "   I wrote the original writeup in the middle of the night after many hours of coding. Hence, I forgot some details of the development of the CNN model.\n",
    "   Much of the development of this project was spent on a bug hunt. The bug ended up being that I was always looking at the original data set, and not each new data set. \"No matter what I do the model never improves!\" Well, \"D'oh! How could it? The original data set was insufficient, and never changed. While trying to figure this out I modified that model many times. Hence the deviation from the Nvidia CNN model. One of the papers I looked at used max-pooling between each convolution layer. This was one of the last changes I made. (Referenced paper: \"Max-Pooling Convolution Neural Networks for Vision-based Hand Gesture Recognition\" by Jawad Nai, et al.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Attempts to Reduce Overfitting in the Model.\n",
    "\n",
    "   I trained the model on many different data sets. As new data sets were added I judged the effects on the simulator.Discussions on forums strongly suggested that a minimum of 100,000 images were needed. I drove the car around the track several more times in both directions, plus added several loops of 'recovery driving'. Also, all images and steering measurements were reversed to double the number of images. \n",
    "   ADDENDUM:\n",
    "   As part of my second submission I added a dropout layer after the first convolution. This made the outcome much worse. I then moved the dropout layer to after the last convolution. This worked. My best guess as to why is that the model needs to digest all of the data for the first few steps in order to get the model on \"the right track\". After that data can be removed with no ill affect. \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Model Parameter Tuning\n",
    "\n",
    "   The model used an Adam Optimizer , so the learning rate did not have to be adjusted manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Appropriate Training Data \n",
    "\n",
    "   I used only images from the center camera, and augemented images derived from the center camera.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Solution Design Approach\n",
    "\n",
    "   My original attempt was to use the model built during the lesson. This proven insufficient. Next I used the architecture in the Nvidia paper. In the end it worked. As I modified the design I would run simulations to judge the effects.\n",
    "   Initially I tried using just Model.fit. This caused memory failures once my data sets became large enough. It took several days to get Model.fit_generator to work. \n",
    "   \n",
    "   ADDENDUM:\n",
    "   In an effort to improve computing time I cropped less useful parts of the images out. After several trials I settled on removing the top row of 75 pixels, the bottom 25, and 20 from the left and right sides. A sample original and cropped version are shown below.\n",
    "   \n",
    "   <img src=\"cropped.png\"> \n",
    "   <img src=\"original.png\"> \n",
    "   \n",
    "   The histogram below shows an even distribution of steering angles. If all test loops had been in one direction there would have been an imbalence of turns to one direction. By including some data gathering loops going in the opposite direction and augmenting the data with mirror images I have producted a balenced data set.\n",
    "    \n",
    "   <img src=\"histogram.png\"> \n",
    "   \n",
    "   \n",
    "   On each run I chose 10 Epochs. As the last three cases that I ran show the \"sweet spot\" moves around. In the first case it appears to be after Epoch 9, 2nd case after epoch 9, and 3rd after epoch 7. This averages out to epoch 8 being the sweet spot. So a better model could be produced by only doing 8 epochs.\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "   Final model from 1st submission\n",
    "   \n",
    "\n",
    "Epoch 1/10\n",
    "131/130 [==============================] - 715s - loss: 0.0124 - val_loss: 0.0081\n",
    "Epoch 2/10\n",
    "131/130 [==============================] - 691s - loss: 0.0070 - val_loss: 0.0078\n",
    "Epoch 3/10\n",
    "131/130 [==============================] - 714s - loss: 0.0064 - val_loss: 0.0073\n",
    "Epoch 4/10\n",
    "131/130 [==============================] - 709s - loss: 0.0055 - val_loss: 0.0069\n",
    "Epoch 5/10\n",
    "131/130 [==============================] - 687s - loss: 0.0050 - val_loss: 0.0067\n",
    "Epoch 6/10\n",
    "131/130 [==============================] - 688s - loss: 0.0042 - val_loss: 0.0067\n",
    "Epoch 7/10\n",
    "131/130 [==============================] - 685s - loss: 0.0038 - val_loss: 0.0065\n",
    "Epoch 8/10\n",
    "131/130 [==============================] - 1774s - loss: 0.0030 - val_loss: 0.0065\n",
    "Epoch 9/10\n",
    "131/130 [==============================] - 687s - loss: 0.0026 - val_loss: 0.0058\n",
    "Epoch 10/10\n",
    "131/130 [==============================] - 686s - loss: 0.0022 - val_loss: 0.0065\n",
    "dict_keys(['loss', 'val_loss'])\n",
    "\n",
    "\n",
    "April 1 2017 After addition of drop out layer after 1st convolution layer\n",
    "\n",
    "\n",
    "Epoch 1/10\n",
    "131/130 [==============================] - 737s - loss: 0.0134 - val_loss: 0.0095\n",
    "Epoch 2/10\n",
    "131/130 [==============================] - 721s - loss: 0.0076 - val_loss: 0.0078\n",
    "Epoch 3/10\n",
    "131/130 [==============================] - 720s - loss: 0.0071 - val_loss: 0.0079\n",
    "Epoch 4/10\n",
    "131/130 [==============================] - 719s - loss: 0.0065 - val_loss: 0.0076\n",
    "Epoch 5/10\n",
    "131/130 [==============================] - 718s - loss: 0.0062 - val_loss: 0.0070\n",
    "Epoch 6/10\n",
    "131/130 [==============================] - 723s - loss: 0.0057 - val_loss: 0.0078\n",
    "Epoch 7/10\n",
    "131/130 [==============================] - 735s - loss: 0.0052 - val_loss: 0.0066\n",
    "Epoch 8/10\n",
    "131/130 [==============================] - 754s - loss: 0.0048 - val_loss: 0.0066\n",
    "Epoch 9/10\n",
    "131/130 [==============================] - 722s - loss: 0.0043 - val_loss: 0.0065\n",
    "Epoch 10/10\n",
    "131/130 [==============================] - 731s - loss: 0.0037 - val_loss: 0.0066\n",
    "dict_keys(['val_loss', 'loss'])\n",
    "\n",
    "\n",
    "7:46pm after moving dropout layer to end of convolutions. This is the final model for 2nd submission\n",
    "\n",
    "\n",
    "Epoch 1/10\n",
    "131/130 [==============================] - 703s - loss: 0.0125 - val_loss: 0.0074\n",
    "Epoch 2/10\n",
    "131/130 [==============================] - 700s - loss: 0.0077 - val_loss: 0.0077\n",
    "Epoch 3/10\n",
    "131/130 [==============================] - 700s - loss: 0.0071 - val_loss: 0.0065\n",
    "Epoch 4/10\n",
    "131/130 [==============================] - 700s - loss: 0.0064 - val_loss: 0.0062\n",
    "Epoch 5/10\n",
    "131/130 [==============================] - 699s - loss: 0.0058 - val_loss: 0.0062\n",
    "Epoch 6/10\n",
    "131/130 [==============================] - 701s - loss: 0.0054 - val_loss: 0.0064\n",
    "Epoch 7/10\n",
    "131/130 [==============================] - 701s - loss: 0.0049 - val_loss: 0.0058\n",
    "Epoch 8/10\n",
    "131/130 [==============================] - 726s - loss: 0.0044 - val_loss: 0.0059\n",
    "Epoch 9/10\n",
    "131/130 [==============================] - 733s - loss: 0.0038 - val_loss: 0.0059\n",
    "Epoch 10/10\n",
    "131/130 [==============================] - 741s - loss: 0.0034 - val_loss: 0.0062\n",
    "dict_keys(['loss', 'val_loss'])\n",
    "\n",
    "<img src=\"mean square.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5. Model.fit_generator\n",
    "\n",
    "   Keras is currently in a awkward position as it migrates from Keras 1.0 to Keras 2.0. I found that fit_generator did not perform as advertised. The problem lies in the batch size. The old argument \"samples_per_epoch\" and \"nb_val_samples\" were designed to take the Number of Iterations per Batch. In Keras 2.0 these arguments are replaced with \"steps_per_epoch\" and \"validation_steps\". These want the Number of Batches per Epoch as their value. Supposedly you can still use the old arguments. You can, but they are now just the two new arguments renamed. Ergo, you must provide the Number of Batches per Epoch instead of the Number of Iterations per Batch. \n",
    "   This is the problem that caused so many people to have fit_generator running ridiculously slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Final Model Architecture\n",
    "\n",
    "   To minimize the compute cycles needed I cropped the images by 20 pixels on each side, 20 on the bottom (removing the\n",
    "hood of the car), and 75 on the top (removing the sky and trees).I used three convolution layers of depths 16, 32, and 64. Each with 3X3 filters. After each convolution I used a max pooled layer. I flattened the output the used reducing Dense layers of 400, 100, 20, and 1. To reduce nonlinearity I used activated RELU on each convolution and each Dense except the last one.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Creating the Training Set\n",
    "   I recorded at least five loops around the track in each direction plus an augmented set of mirror images of that set. I used only the center camera. I suspected that the side camera's were unnecessary. After all, humans can drive with only a center view.\n",
    "   I also had at least one loop of 'recovery' data. I would drive too far to the side then record myself driving back to center.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Conclusion.\n",
    "\n",
    "   This was another awesome project!  It took far longer than I expected, but that is programming. My main suggestion is to alter the lesson notes to reflect the problem with fit_generator. This should save students considerable time.\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:PY3env-gpu]",
   "language": "python",
   "name": "conda-env-PY3env-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
